# Token Forensics Report

**Generated:** 2026-02-26T06:37:58.044480+00:00
**Report Version:** 1.0.0

---

## Executive Summary

This report provides a comprehensive forensic analysis of the dataset tokenization
and quality metrics. The analysis covers 169,397
samples with 231,608,618 total tokens.

### Key Metrics

| Metric | Value |
|--------|-------|
| Total Samples | 169,397 |
| Total Tokens | 231,608,618 |
| Avg Tokens/Sample | 0.00 |
| Sources | 0 |

## Context Window Analysis

| Context Size | Samples Fitting | Truncation Rate |
|--------------|-----------------|-----------------|
| 4K | 97.0% | 3.0% |
| 8K | 98.4% | 1.6% |
| 32K | 100.0% | 0.0% |

### Key Findings

- **4K** context is the most restrictive, fitting only 97.0% of samples

## Recommendations

Based on the forensic analysis, the following recommendations are provided:

### Dataset Usage

1. **Context Window Selection:** Consider the target model's context length when filtering.
2. **Quality Thresholds:** Samples with `quality_score < 0.3` should be reviewed before inclusion.
3. **Deduplication:** Remove exact duplicates; consider near-duplicate filtering for diversity.

### Safety Considerations

1. Review samples flagged with high safety risk scores (>=0.75)
2. Consider PII scrubbing for samples with detected patterns.
3. Validate language assignments for non-English content.

---

*Report generated by Token Forensics Validation System*