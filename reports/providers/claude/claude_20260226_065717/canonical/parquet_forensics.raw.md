# Token Forensics Report

**Generated:** 2026-02-26T07:22:12.104744+00:00
**Report Version:** 1.0.0

---

## Executive Summary

This report provides a comprehensive forensic analysis of the dataset tokenization
and quality metrics. The analysis covers 5,589
samples with 6,060,574 total tokens.

### Key Metrics

| Metric | Value |
|--------|-------|
| Total Samples | 5,589 |
| Total Tokens | 6,060,574 |
| Avg Tokens/Sample | 0.00 |
| Sources | 0 |

## Context Window Analysis

| Context Size | Samples Fitting | Truncation Rate |
|--------------|-----------------|-----------------|
| 4K | 99.2% | 0.8% |
| 8K | 99.9% | 0.1% |
| 32K | 100.0% | 0.0% |

### Key Findings

- **4K** context is the most restrictive, fitting only 99.2% of samples

## Recommendations

Based on the forensic analysis, the following recommendations are provided:

### Dataset Usage

1. **Context Window Selection:** Consider the target model's context length when filtering.
2. **Quality Thresholds:** Samples with `quality_score < 0.3` should be reviewed before inclusion.
3. **Deduplication:** Remove exact duplicates; consider near-duplicate filtering for diversity.

### Safety Considerations

1. Review samples flagged with high safety risk scores (>=0.75)
2. Consider PII scrubbing for samples with detected patterns.
3. Validate language assignments for non-English content.

---

*Report generated by Token Forensics Validation System*