# Token Forensics Report

**Generated:** 2026-02-17T10:10:24.048647+00:00
**Report Version:** 1.0.0

---

## Executive Summary

This report provides a comprehensive forensic analysis of the dataset tokenization
and quality metrics. The analysis covers 10,000
samples with 9,405,521 total tokens.

### Key Metrics

| Metric | Value |
|--------|-------|
| Total Samples | 10,000 |
| Total Tokens | 9,405,521 |
| Avg Tokens/Sample | 940.55 |
| Sources | 4 |

## Tokenizer Comparison

| Tokenizer | Total Tokens | Avg/Sample | Compression |
|-----------|--------------|------------|-------------|
| code-llama | 2,260,177 | 921.0 | 0.0988 |
| gpt2 | 2,417,237 | 961.9 | 0.1000 |
| llama-2 | 2,309,734 | 910.4 | 0.1061 |
| mistral | 2,418,373 | 968.9 | 0.0999 |

## Context Window Analysis

| Context Size | Samples Fitting | Truncation Rate |
|--------------|-----------------|-----------------|
| 4K | 84.9% | 39.8% |
| 8K | 95.1% | 9.4% |
| 32K | 98.8% | 0.0% |

### Key Findings

- **4K** context is the most restrictive, fitting only 84.9% of samples

## Quality Distribution

### Quality Score

| Range | Count |
|-------|-------|
| 0.0-0.2 | 272 |
| 0.2-0.4 | 1,072 |
| 0.4-0.6 | 2,290 |
| 0.6-0.8 | 3,429 |
| 0.8-1.0 | 2,937 |

### Entropy Score

| Range | Count |
|-------|-------|
| 0.0-0.2 | 76 |
| 0.2-0.4 | 765 |
| 0.4-0.6 | 2,508 |
| 0.6-0.8 | 4,044 |
| 0.8-1.0 | 2,607 |

### Repetition Score

| Range | Count |
|-------|-------|
| 0.0-0.2 | 6,802 |
| 0.2-0.4 | 2,608 |
| 0.4-0.6 | 370 |
| 0.6-0.8 | 90 |
| 0.8-1.0 | 130 |

## Duplication Analysis

- **Exact Duplicates:** 518 samples (5.18%) in 332 groups
- **Near Duplicates:** 977 samples (9.77%) in 615 clusters

## Anomaly Summary

- **Total Flagged Samples:** 0
- **Unique Anomaly Types:** 0

### Anomaly Type Breakdown

| Anomaly Type | Count |
|--------------|-------|

## Safety Risk Summary

| Risk Level | Count |
|------------|-------|
| Low Risk 0 0.25 | 9,907 |
| Medium Risk 0.25 0.5 | 43 |
| High Risk 0.5 0.75 | 0 |
| Critical Risk 0.75 1 | 50 |

## Recommendations

Based on the forensic analysis, the following recommendations are provided:

### Dataset Usage

1. **Context Window Selection:** Consider the target model's context length when filtering.
2. **Quality Thresholds:** Samples with `quality_score < 0.3` should be reviewed before inclusion.
3. **Deduplication:** Remove exact duplicates; consider near-duplicate filtering for diversity.

### Safety Considerations

1. Review samples flagged with high safety risk scores (>=0.75)
2. Consider PII scrubbing for samples with detected patterns.
3. Validate language assignments for non-English content.

---

*Report generated by Token Forensics Validation System*